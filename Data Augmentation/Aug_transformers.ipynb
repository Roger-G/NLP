{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0226 21:10:31.351524 140736292361152 file_utils.py:39] PyTorch version 1.4.0 available.\n",
      "I0226 21:10:32.052206 140736292361152 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "import pysnooper\n",
    "import spacy \n",
    "  \n",
    "# Load English tokenizer, tagger,  \n",
    "# parser, NER and word vectors \n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0226 21:10:33.761631 140736292361152 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /Users/gaojie/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 版本： 1.4.0\n"
     ]
    }
   ],
   "source": [
    "#@title\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-uncased\"  # 指定 BERT-BASE 預訓練模型\n",
    "\n",
    "# 取得此預訓練模型所使用的 tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "\n",
    "# clear_output()\n",
    "print(\"PyTorch 版本：\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.insert(0, \"/Users/gaojie/HCID-3/NLP/Data/Treebank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "absFilePath = os.path.abspath('')\n",
    "# print(absFilePath)\n",
    "# print(dirname)\n",
    "# filename = os.path.join(dirname, '/Treebank/datasetSentences.txt')\n",
    "path = os.path.join(absFilePath, '../'+ 'datasetSentences' + '.txt')\n",
    "# with open(path) as f:\n",
    "#     test = list(csv.reader(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = open(path, \"r\").readlines()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalizeString(s):\n",
    "# #         s = s.lower()\n",
    "#         s = s.replace(r\"<br />\",r\" \")\n",
    "#         # s = re.sub(' +',' ',s)\n",
    "#         s = s.replace(r'(\\W)(?=\\1)', '')\n",
    "#         s = s.replace(r\"([.!?])\", r\" \\1\")\n",
    "#         s = s.replace(r\"[^a-zA-Z.!?]+\", r\" \")\n",
    "\n",
    "#         return s\n",
    "# # len(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"/Users/gaojie/HCID-3/NLP/Data/Treebank/datasetSentences.txt\", \"r\") as file:\n",
    "#     text = file.readlines()\n",
    "    \n",
    "# label = [re.split(r\"[,\\n \\-!?:]+\", data[i])[1] for i in range(len(data))]\n",
    "\n",
    "# with open(\"/Users/gaojie/HCID-3/NLP/Data/Treebank/datasetSentences.txt\", \"r\") as file:\n",
    "#     text = file.readlines()\n",
    "    \n",
    "# text = text[1:30]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(text)):\n",
    "#     text[i] = text[i].split()[1:]\n",
    "#     text[i] = ' '.join([str(elem) for elem in data1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "tokens ['hi', 'my', 'name', 'is', 'apple', ',', 'and', 'i', 'like', 'apple']\n",
      "is\n",
      "like\n"
     ]
    }
   ],
   "source": [
    "text =  'Hi my name is apple, and I like apple'\n",
    "doc = nlp(text) \n",
    "doc = list(doc)\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(type(tokens))\n",
    "\n",
    "# for idx, token in enumerate(doc): \n",
    "#     if token.pos_ == 'VERB':\n",
    "#         doc[idx] = '[MASK]'\n",
    "        \n",
    "\n",
    "#     print(token, token.pos_) \n",
    "#     print('idx', idx)\n",
    "  # You want list of Verb tokens \n",
    "ver_list = [token.text for token in doc if token.pos_ == \"VERB\"]\n",
    "print('tokens', tokens)\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ == 'VERB':\n",
    "        print(token)\n",
    "\n",
    "for idx, token in enumerate(doc):\n",
    "    if token.pos_ == 'VERB':\n",
    "        doc[idx] = '[MASK]'\n",
    "\n",
    "#         tokens.index(token) = '[MASK]'\n",
    "\n",
    "# tokens\n",
    "# print(\"Verbs:\", [token.text for token in doc if token.pos_ == \"VERB\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_text = os.path.join(absFilePath, '../'+ 'datasetSentences' + '.txt')\n",
    "text = open(path_text, \"r\").readlines()[1:11]\n",
    "# text.remove(text[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0\n",
      "len of text[i] 37\n",
      "text_i 1\tThe Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\n",
      "\n",
      "i 1\n",
      "len of text[i] 38\n",
      "text_i 2\tThe gorgeously elaborate continuation of `` The Lord of the Rings '' trilogy is so huge that a column of words can not adequately describe co-writer\\/director Peter Jackson 's expanded vision of J.R.R. Tolkien 's Middle-earth .\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['3\\tEffective but too-tepid biopic\\n',\n",
       " '4\\tIf you sometimes like to go to the movies to have fun , Wasabi is a good place to start .\\n',\n",
       " \"5\\tEmerges as something rare , an issue movie that 's so honest and keenly observed that it does n't feel like one .\\n\",\n",
       " '6\\tThe film provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\\n',\n",
       " '7\\tOffers that rare combination of entertainment and education .\\n',\n",
       " '8\\tPerhaps no picture ever made has more literally showed that the road to hell is paved with good intentions .\\n',\n",
       " \"9\\tSteers turns in a snappy screenplay that curls at the edges ; it 's so clever you want to hate it .\\n\",\n",
       " '10\\tBut he somehow pulls it off .\\n']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "be_remove = []\n",
    "path_text = os.path.join(absFilePath, '../'+ 'datasetSentences' + '.txt')\n",
    "text = open(path_text, \"r\").readlines()[1:11]\n",
    "text_copy = text.copy()\n",
    "for i in range(len(text)):\n",
    "#     print('len_texti', len(text[i].split()))\n",
    "    if len(text[i].split()) > 30:\n",
    "        be_remove.append(i)\n",
    "        print('i', i)\n",
    "#         print('len_text', len(text))\n",
    "        print('len of text[i]', len(text[i].split()))\n",
    "        print('text_i', text[i])\n",
    "#     text.pop(be_remove)\n",
    "        text_copy.remove(text[i])\n",
    "        \n",
    "\n",
    "text_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"1\\tThe Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\\n\",\n",
       " \"2\\tThe gorgeously elaborate continuation of `` The Lord of the Rings '' trilogy is so huge that a column of words can not adequately describe co-writer\\\\/director Peter Jackson 's expanded vision of J.R.R. Tolkien 's Middle-earth .\\n\",\n",
       " '3\\tEffective but too-tepid biopic\\n',\n",
       " '4\\tIf you sometimes like to go to the movies to have fun , Wasabi is a good place to start .\\n',\n",
       " \"5\\tEmerges as something rare , an issue movie that 's so honest and keenly observed that it does n't feel like one .\\n\",\n",
       " '6\\tThe film provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\\n',\n",
       " '7\\tOffers that rare combination of entertainment and education .\\n',\n",
       " '8\\tPerhaps no picture ever made has more literally showed that the road to hell is paved with good intentions .\\n',\n",
       " \"9\\tSteers turns in a snappy screenplay that curls at the edges ; it 's so clever you want to hate it .\\n\",\n",
       " '10\\tBut he somehow pulls it off .\\n']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = os.path.join(absFilePath, '../'+ 'datasetSentences' + '.txt')\n",
    "import re\n",
    "\n",
    "class Prepro_Data(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenizer, nlp):\n",
    "        path_text = os.path.join(absFilePath, '../'+ 'datasetSentences' + '.txt')\n",
    "        path_label = os.path.join(absFilePath, '../'+ 'datasetSplit' + '.txt')\n",
    "        self.text = open(path_text, \"r\").readlines()[1:1001]\n",
    "        self.label = open(path_label, \"r\").readlines()[1:1001]\n",
    "        self.label = [re.split(r\"[,\\n \\-!?:]+\", self.label[i])[1] for i in range(len(self.label))]\n",
    "        \n",
    "        \n",
    "        text_copy = self.text.copy()\n",
    "        label = self.label.copy()\n",
    "\n",
    "        \n",
    "        for i in range(len(text_copy)):\n",
    "            if len(text_copy[i].split()) > 30:\n",
    "                \n",
    "                self.label.remove(label[i])\n",
    "                self.text.remove(text_copy[i])\n",
    "\n",
    "        \n",
    "        len(self.text)\n",
    "\n",
    "#         print('self.label', self.label)\n",
    "#         print('df_text', self.text)\n",
    "        \n",
    "        for i in range(len(self.text)):\n",
    "            \n",
    "            self.text[i] = self.text[i].split()[1:]\n",
    "            \n",
    "            self.text[i] = ' '.join([str(elem) for elem in self.text[i]])\n",
    "\n",
    "        \n",
    "#         print('after df_text', self.text)\n",
    "        \n",
    "        self.label_map = {'1': 0, '2': 1}\n",
    "        self.tokenizer = tokenizer\n",
    "        self.nlp = nlp\n",
    "        self.len = len(self.text)\n",
    "    \n",
    "    def __getitem__(self, idx, sign=None):\n",
    "        \n",
    "        text, label = self.text[idx], self.label[idx]\n",
    "        label_id = self.label_map[label]\n",
    "#         print('text', text)\n",
    "#         print('label', label)\n",
    "        word_pieces = ['[CLS]']\n",
    "    \n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "#         tokens = list(self.nlp(text))\n",
    "#         print('tokens', tokens)\n",
    "        \n",
    "        if sign == 'Augment':\n",
    "            \n",
    "            for idx, token in enumerate(tokens):\n",
    "\n",
    "                if token.pos_ == 'VERB':\n",
    "                    tokens[idx] = '[MASK]'\n",
    "                \n",
    "        word_pieces += tokens + ['[SEP]']\n",
    "#         print('word_pieces', word_pieces)\n",
    "#         len = len(word_pieces)\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "#         print('ids', ids)\n",
    "        label_tensor = torch.tensor(label_id)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        segments_tensor = torch.zeros_like(tokens_tensor)  # (1, seq_len)\n",
    "        \n",
    "#         mask_tensors = mask_tensors.masked_fill(tokens_tensors != 0, 1)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    \n",
    "    def  __len__(self):\n",
    "        \n",
    "        return self.len\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = Prepro_Data(tokenizer = tokenizer, nlp = nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 101, 4107, 2008, 4678, 5257, 1997, 4024, 1998, 2495, 1012,  102]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1a51d47320>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def create_mini_batch(samples):\n",
    "    \n",
    "    \n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "        \n",
    "    else:\n",
    "        label_ids = None\n",
    "        \n",
    "#     print('before pad tokens_tensors', tokens_tensors)\n",
    "    tokens_tensors = pad_sequence(tokens_tensors,\n",
    "                                 batch_first=True)\n",
    "#     print('tokens_tensors', tokens_tensors)\n",
    "    segments_tensors = pad_sequence(segments_tensors,\n",
    "                                   batch_first=True)\n",
    "    \n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape,\n",
    "                               dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)\n",
    "\n",
    "trainloader\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0226 22:48:29.498169 140736292361152 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /Users/gaojie/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "I0226 22:48:29.502207 140736292361152 configuration_utils.py:168] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0226 22:48:30.087244 140736292361152 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /Users/gaojie/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I0226 22:48:33.436153 140736292361152 modeling_utils.py:405] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0226 22:48:33.436953 140736292361152 modeling_utils.py:408] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name            module\n",
      "----------------------\n",
      "bert:embeddings\n",
      "bert:encoder\n",
      "bert:pooler\n",
      "dropout         Dropout(p=0.1, inplace=False)\n",
      "classifier      Linear(in_features=768, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "NUM_LABELS = 2\n",
    "model = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME,\n",
    "                                                     num_labels=NUM_LABELS)\n",
    "print(\"\"\"\n",
    "name            module\n",
    "----------------------\"\"\")\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(f\"{name}:{n}\")\n",
    "    else:\n",
    "        print(\"{:15} {}\".format(name, module))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification acc: 0.04362801377726751\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for data in dataloader:\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to('cuda:0') for t in data if t is not None]\n",
    "                \n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "\n",
    "            outputs = model(input_ids=tokens_tensors,\n",
    "                           token_type_ids=segments_tensors,\n",
    "                           attention_mask=masks_tensors)\n",
    "\n",
    "#             print('outputs', outputs)\n",
    "            logits = outputs[0]\n",
    "\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "                \n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "#                 print('total', total)\n",
    "\n",
    "                correct += (pred == labels).sum().item()\n",
    "\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "\n",
    "            else:\n",
    "\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "                    \n",
    "        if compute_acc:\n",
    "            acc = correct / total\n",
    "            return predictions, acc\n",
    "\n",
    "        return predictions\n",
    "        \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = model.to(device)\n",
    "_, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "print('classification acc:', acc)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data in trainloader:\n",
    "#         tokens_tensors, segments_tensors, \\\n",
    "#         masks_tensors, labels = [t.to(device) for t in data]\n",
    "# #         print('labels',  labels)\n",
    "# #         print('masks_tensors', masks_tensors)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(input_ids=tokens_tensors,\n",
    "#                                 token_type_ids=segments_tensors,\n",
    "#                                 attention_mask=masks_tensors,\n",
    "#                                 labels=labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "EPOCHS = 6\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for data in trainloader:\n",
    "        \n",
    "        tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=tokens_tensors,\n",
    "                        token_type_ids=segments_tensors,\n",
    "                        attention_mask=masks_tensors,\n",
    "                        labels=labels)\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    _, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "    \n",
    "    print('[epoch %d] loss: %.3f, acc:%.3f' % (epoch + 1, running_loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
