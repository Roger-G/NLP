{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0 cpu\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import csv\n",
    "import torch.utils.data as Data\n",
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "import math\n",
    "import torchtext.vocab as Vocab\n",
    "import collections\n",
    "import time\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext import data, datasets\n",
    "PAD, BOS, EOS = '<pad>', '<bos>', '<eos>'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# torch.set_printoptions(profile=8)\n",
    "print(torch.__version__, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class loading_data():\n",
    "    \n",
    "    def __init__(self, max_seq_len, data):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.data = data\n",
    "    # in和out分别是input和output的缩写\n",
    "    # 将一个序列中所有的词记录在all_tokens中以便之后构造词典，然后在该序列后面添加PAD直到序列\n",
    "    # 长度变为max_seq_len，然后将序列保存在all_seqs中\n",
    "    def process_one_seq(self, seq_tokens, all_tokens, all_seqs, max_seq_len):\n",
    "        all_tokens.extend(seq_tokens) # add seq_tokens into one list, like extend the list, [..., seq_tokens].\n",
    "    #     print('all_tokens', len(all_tokens))\n",
    "        seq_tokens += [EOS] + [PAD] * (max_seq_len - len(seq_tokens) - 1)\n",
    "    #     print('after add pad seq_tokens: ', seq_tokens)\n",
    "        all_seqs.append(seq_tokens) # add seq_tokens become a list element, [...,[seq_tokens]].\n",
    "    #     print('all_seqs', all_seqs)\n",
    "\n",
    "    # 使用所有的词来构造词典。并将所有序列中的词变换为词索引后构造Tensor\n",
    "    def build_data(self, all_tokens, all_seqs):\n",
    "\n",
    "    #   collections.Counter(), A counter is a container that stores elements as dictionary keys, and their counts are stored as dictionary values.\n",
    "        tokens_dic = collections.Counter(all_tokens) \n",
    "    #     print('tokens_dic', tokens_dic)\n",
    "        vocab = Vocab.Vocab(tokens_dic, specials = [PAD, BOS, EOS])\n",
    "    #     print('vocab', vocab)\n",
    "        indices = [[vocab.stoi[w] for w in seq] for seq in all_seqs] # use its vocab_dic to represent the sentence. \n",
    "    #     print('length of indices', len(indices))\n",
    "    #     print('indices', torch.tensor(indices))\n",
    "        return vocab, torch.tensor(indices)\n",
    "    \n",
    "    def normalizeString(self, s):\n",
    "        s = s.str.lower()\n",
    "        s = s.str.replace(r\"<br />\",r\" \")\n",
    "        # s = re.sub(' +',' ',s)\n",
    "        s = s.str.replace(r'(\\W)(?=\\1)', '')\n",
    "        s = s.str.replace(r\"([.!?])\", r\" \\1\")\n",
    "        s = s.str.replace(r\"[^a-zA-Z.!?]+\", r\" \")\n",
    "\n",
    "        return s\n",
    "    \n",
    "    def main(self):\n",
    "        \n",
    "        in_tokens, in_seqs = [], []\n",
    "        df = self.data.copy()\n",
    "        df['comment'] = normalizeString(df['comment'])\n",
    "        df.reset_index(inplace=True, drop=True)\n",
    "    #     print(df[:10])\n",
    "        k = 10\n",
    "        target = []\n",
    "        for line in range(k):\n",
    "    #         print('line', line)\n",
    "            in_seq_tokens = df['comment'][line].split(' ')\n",
    "\n",
    "            if len(in_seq_tokens) > self.max_seq_len - 1:\n",
    "                continue  # 如果加上EOS后长于max_seq_len，则忽略掉此样本\n",
    "\n",
    "            self.process_one_seq(in_seq_tokens, in_tokens, in_seqs, self.max_seq_len)\n",
    "            target.append(df.label[line])\n",
    "\n",
    "        in_vocab, in_data = self.build_data(in_tokens, in_seqs) # in_tokens is the list where contains every word, in_seqs is a list where its element are the sentence in French.\n",
    "        return in_vocab, in_data, torch.tensor(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN for Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, drop_prob = 0):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, dropout=drop_prob)\n",
    "        self.dense = nn.Linear(hidden_size, vocab_size)\n",
    "        self.state = None\n",
    "\n",
    "    def forward(self, inputs, state): # inputs: (seq_len, batch), \n",
    "        \n",
    "        embedding = self.embedding(inputs.to(torch.int64))\n",
    "        Y, state = self.rnn(embedding, self.state)\n",
    "        output = self.dense(Y[-1,:,:]) # so this is why we need Y[:, -1, :] 最外层的hidden state, because it already contains all the information of last all words.\n",
    "        return output, state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class augmentation():\n",
    "    \n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, loss_da, corpus_indices, device):\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = vocab\n",
    "        self.loss_da = loss_da\n",
    "        self.corpus_indices = corpus_indices\n",
    "#         self.batch_size = batch_size\n",
    "#         self.num_steps = num_steps\n",
    "    \n",
    "    def data_iter_consecutive(self, corpus_indices, batch_size, num_steps, device=None):\n",
    "        \n",
    "        if device is None:\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        corpus_indices = torch.tensor(corpus_indices, device=device)\n",
    "\n",
    "        data_len = len(corpus_indices)\n",
    "        batch_len = data_len // batch_size\n",
    "        indices = corpus_indices[0: batch_size*batch_len].view(batch_size, batch_len)\n",
    "        epoch_size = (batch_len - 1) // num_steps\n",
    "        for i in range(epoch_size):\n",
    "    #         i = i * num_steps\n",
    "            X = indices[:, i: i + num_steps]\n",
    "            Y = indices[:, i + num_steps + 1] # 错1位取data\n",
    "            yield X, Y\n",
    "            \n",
    "    def train_rnn(self, model, vocab_size, device,\n",
    "                                corpus_indices, char_to_idx,\n",
    "                                num_epochs, num_steps, lr, clipping_theta,\n",
    "                                batch_size):\n",
    "#         device = None\n",
    "#         print(corpus_indices.shape)\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        model.to(device)\n",
    "        state = None\n",
    "        for epoch in range(num_epochs):\n",
    "            l_sum, n, start = 0.0, 0, time.time()\n",
    "            data_iter = self.data_iter_consecutive(corpus_indices, batch_size, num_steps, device) # 相邻采样\n",
    "    #         print('data_iter_consecutive is no errors')\n",
    "            for X, Y in data_iter:\n",
    "                if state is not None:\n",
    "                    # 使用detach函数从计算图分离隐藏状态, 这是为了\n",
    "                    # 使模型参数的梯度计算只依赖一次迭代读取的小批量序列(防止梯度计算开销太大)\n",
    "                    if isinstance (state, tuple): # LSTM, state:(h, c)  \n",
    "                        state = (state[0].detach(), state[1].detach())\n",
    "                    else:   \n",
    "                        state = state.detach()\n",
    "                (output, state) = model(X.permute(1,0), state) # change X shape to (seq, batch_size) instead of (batch_size, seq)\n",
    "                # output 的形状是(batch_size, vocab) Y(batch_size)\n",
    "                l = loss_da(output, Y.long())\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                l.backward()\n",
    "                optimizer.step()\n",
    "                l_sum += l.item() * Y.shape[0]\n",
    "                n += Y.shape[0]\n",
    "\n",
    "        corpus = self.predict_rnn_pytorch(corpus_indices, model, vocab_size, device)\n",
    "        return corpus\n",
    "    \n",
    "    def predict_rnn_pytorch(self, corpus, model, vocab_size, device):\n",
    "\n",
    "        output = []\n",
    "        for i in range(3, corpus.shape[0], 4):\n",
    "\n",
    "            X = corpus[i-3:i]\n",
    "            state = None\n",
    "            if state is not None:\n",
    "                if isinstance(state, tuple): # LSTM, state:(h, c)  \n",
    "                    state = (state[0].to(device), state[1].to(device))\n",
    "                else:   \n",
    "                    state = state.to(device)\n",
    "            X = torch.tensor(torch.unsqueeze(X,0), device=device).view(3, 1) # X is a list need to become a two demission array then could be change to(3,1 )(seq, batch_size)\n",
    "            (Y, state) = model(X, state)  # Y (batch_size, vocab)\n",
    "            corpus[i] = int(Y.argmax(dim=1).item())\n",
    "\n",
    "        return corpus\n",
    "    \n",
    "    def main(self):\n",
    "        \n",
    "#         if device is None:\n",
    "#             device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        start = time.time()\n",
    "        model = RNNModel(self.vocab_size, self.embed_size, self.hidden_size).to(self.device)\n",
    "        loss_da = nn.CrossEntropyLoss()\n",
    "        corpus = self.corpus_indices\n",
    "\n",
    "        for i in range(corpus.shape[0]):\n",
    "\n",
    "            corpus_indice, tokens_dic, vocab_size = corpus[i], self.vocab, len(self.vocab)\n",
    "            num_epochs, batch_size, lr, clipping_theta, num_steps= 20, 8, 1e-3, 1e-2, 4 # 注意这里的学习率设置\n",
    "\n",
    "            corpus[i] = self.train_rnn(model, vocab_size, self.device,\n",
    "                                    corpus_indice, tokens_dic, num_epochs, num_steps, \n",
    "                                    lr, clipping_theta, batch_size)\n",
    "            \n",
    "            if (i+1) % 5 == 0:\n",
    "                    print('i %d, time %.2f sec' % (\n",
    "                        i+1, time.time() - start))\n",
    "        return corpus        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_indices_ori = Data.TensorDataset(corpus_indices, label)\n",
    "# corpus_indices_da = Data.TensorDataset(corpus, label)\n",
    "# corpus_indices_classification = Data.TensorDataset(corpus_indices_valid, label_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     use_cuda = True\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "#     use_cuda = False\n",
    "# vocabLimit = vocab_size\n",
    "# input_dim = 300\n",
    "# max_sequence_len = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_length = len(vocab)\n",
    "# vocab_length_valid = len(vocab_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "        \n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_DIM = vocab_length\n",
    "# EMBEDDING_DIM = 100\n",
    "# HIDDEN_DIM = 256\n",
    "# OUTPUT_DIM = 1\n",
    "\n",
    "# model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class classification():\n",
    "    \n",
    "    def __init__(self, vocab, vocab_vaild, training_data_ori, training_data_aug, validation_data, num_epochs):\n",
    "        \n",
    "        self.vocab_length = len(vocab)\n",
    "        self.vocab_length_valid = len(vocab_valid)\n",
    "        self.traing_data_ori = training_data_ori\n",
    "        self.traing_data_aug = training_data_aug\n",
    "        self.validation_data = validation_data\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "    \n",
    "    \n",
    "    def binary_accuracy(self, preds, y):\n",
    "        \"\"\"\n",
    "        Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "        \"\"\"\n",
    "        #round predictions to the closest integer\n",
    "        rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "        correct = (rounded_preds == y).float() #convert into float for division \n",
    "        acc = correct.sum() / len(correct)\n",
    "        return acc\n",
    "\n",
    "    def train(self, model, corpus_indices, optimizer, criterion):\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "            use_cuda = True\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            use_cuda = False\n",
    "    \n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        data_iter = Data.DataLoader(corpus_indices, batch_size = 4, shuffle=True)\n",
    "\n",
    "        for X, Y in data_iter:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if use_cuda:\n",
    "                X = Variable(torch.cuda.LongTensor(X))\n",
    "            else:\n",
    "                X = Variable(torch.LongTensor(X))\n",
    "\n",
    "            y_pred = model(X.permute(1, 0)).squeeze(1)\n",
    "\n",
    "    #         print('y_pred', y_pred)\n",
    "            loss = criterion(y_pred,Y.to(device).float())\n",
    "            acc = self.binary_accuracy(y_pred, Y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "        return epoch_loss / len(data_iter), epoch_acc / len(data_iter)\n",
    "    \n",
    "    def evaluate(self, model, iterator, optimizer, criterion):\n",
    "\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "\n",
    "        model.eval()\n",
    "        data_iter = Data.DataLoader(iterator, batch_size = 4, shuffle=True)\n",
    "        \n",
    "        for X, Y in data_iter:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(X.permute(1, 0)).squeeze(1)\n",
    "\n",
    "            loss = criterion(y_pred,Y.to(device).float())\n",
    "            acc = self.binary_accuracy(y_pred, Y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            \n",
    "        return epoch_loss / len(data_iter), epoch_acc / len(data_iter)\n",
    "    \n",
    "    def epoch_time(self, start_time, end_time):\n",
    "        \n",
    "        elapsed_time = end_time - start_time\n",
    "        elapsed_mins = int(elapsed_time / 60)\n",
    "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "        return elapsed_mins, elapsed_secs\n",
    "    \n",
    "    def main(self):\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "            use_cuda = True\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            use_cuda = False\n",
    "        \n",
    "        INPUT_DIM = self.vocab_length\n",
    "        EMBEDDING_DIM = 100\n",
    "        HIDDEN_DIM = 256\n",
    "        OUTPUT_DIM = 1\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
    "        model_for_ori = model\n",
    "        model_for_aug = model\n",
    "        model = model.to(device)\n",
    "        criterion = criterion.to(device)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "        N_EPOCHS = self.num_epochs\n",
    "\n",
    "        best_valid_loss = float('inf')\n",
    "\n",
    "        for epoch in range(N_EPOCHS):\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            train_loss_ori, train_acc_ori = self.train(model_for_ori, self.traing_data_ori, optimizer, criterion)\n",
    "            valid_loss_ori, valid_acc_ori = self.evaluate(model_for_ori, self.validation_data, optimizer, criterion)\n",
    "            train_loss_aug, train_acc_aug = self.train(model_for_aug, self.traing_data_aug, optimizer, criterion)\n",
    "            valid_loss_aug, valid_acc_aug = self.evaluate(model_for_aug, self.validation_data, optimizer, criterion)\n",
    "\n",
    "            end_time = time.time()\n",
    "\n",
    "            epoch_mins, epoch_secs = self.epoch_time(start_time, end_time)\n",
    "\n",
    "            if valid_loss_aug < best_valid_loss:\n",
    "                best_valid_loss = valid_loss_aug\n",
    "                torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "\n",
    "            if (epoch + 1) % 4 == 0:\n",
    "    #             print('epoch %d, perplexity %f, time %.2f sec' % (\n",
    "    #                 epoch + 1, perplexity, time.time() - start))\n",
    "\n",
    "                print(f'Epoch: {epoch+1:2} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "                print('Before data augmentation')\n",
    "                print(f'\\tTrain Loss: {train_loss_ori:.3f} | Train Acc: {train_acc_ori*100:.2f}%')\n",
    "                print(f'\\t Val. Loss: {valid_loss_ori:.3f} |  Val. Acc: {valid_acc_ori*100:.2f}%')\n",
    "                print('After data augmentation')\n",
    "                print(f'\\tTrain Loss: {train_loss_aug:.3f} | Train Acc: {train_acc_aug*100:.2f}%')\n",
    "                print(f'\\t Val. Loss: {valid_loss_aug:.3f} |  Val. Acc: {valid_acc_aug*100:.2f}%')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_class(df, embed_size, hidden_size, loss_da): \n",
    "    \n",
    "    df_train, df_test = train_test_split(df, test_size=0.3, random_state=41)\n",
    " \n",
    "    for i in [0.1, 0.2]:\n",
    "        \n",
    "        print(\" %.0f%% of dataset \" % (100 * (i)) )\n",
    "        df_new = resample(df, n_samples=int(i*len(df)),random_state=1, replace=False)\n",
    "        vocab, corpus_indices, label = loading_data(300, df_train).main()\n",
    "        vocab_valid, corpus_indices_valid, label_valid = loading_data(300, df_test).main()\n",
    "          \n",
    "        corpus = augmentation(embed_size, hidden_size, vocab_size, loss_da, corpus_indices, device = device).main()\n",
    "        \n",
    "        corpus_indices_ori = Data.TensorDataset(corpus_indices, label)\n",
    "        corpus_indices_classification = Data.TensorDataset(corpus_indices_valid, label_valid)\n",
    "        corpus_indices_da = Data.TensorDataset(corpus, label)\n",
    "        \n",
    "        to_class = classification(vocab, vocab_valid, corpus_indices_ori, corpus_indices_da, corpus_indices_classification, 8)\n",
    "\n",
    "        to_class.main()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10% of dataset \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gaojie/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/Users/gaojie/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 5, time 5.03 sec\n",
      "Epoch:  4 | Epoch Time: 0m 0s\n",
      "Before data augmentation\n",
      "\tTrain Loss: 0.704 | Train Acc: 41.67%\n",
      "\t Val. Loss: 0.706 |  Val. Acc: 37.50%\n",
      "After data augmentation\n",
      "\tTrain Loss: 0.702 | Train Acc: 41.67%\n",
      "\t Val. Loss: 0.704 |  Val. Acc: 37.50%\n",
      "Epoch:  8 | Epoch Time: 0m 0s\n",
      "Before data augmentation\n",
      "\tTrain Loss: 0.698 | Train Acc: 12.50%\n",
      "\t Val. Loss: 0.690 |  Val. Acc: 62.50%\n",
      "After data augmentation\n",
      "\tTrain Loss: 0.694 | Train Acc: 54.17%\n",
      "\t Val. Loss: 0.690 |  Val. Acc: 62.50%\n",
      " 20% of dataset \n",
      "i 5, time 5.60 sec\n",
      "Epoch:  4 | Epoch Time: 0m 0s\n",
      "Before data augmentation\n",
      "\tTrain Loss: 0.693 | Train Acc: 54.17%\n",
      "\t Val. Loss: 0.687 |  Val. Acc: 62.50%\n",
      "After data augmentation\n",
      "\tTrain Loss: 0.689 | Train Acc: 58.33%\n",
      "\t Val. Loss: 0.686 |  Val. Acc: 62.50%\n",
      "Epoch:  8 | Epoch Time: 0m 0s\n",
      "Before data augmentation\n",
      "\tTrain Loss: 0.685 | Train Acc: 58.33%\n",
      "\t Val. Loss: 0.679 |  Val. Acc: 62.50%\n",
      "After data augmentation\n",
      "\tTrain Loss: 0.685 | Train Acc: 58.33%\n",
      "\t Val. Loss: 0.678 |  Val. Acc: 62.50%\n"
     ]
    }
   ],
   "source": [
    "     \n",
    "df = pd.read_csv(\"./IMDB Dataset.csv\", names=['comment', 'label'], header=0, encoding='utf-8')\n",
    "df['label'] = df['label'].apply(lambda x: 1 if x=='positive' else 0)\n",
    "embed_size = 5\n",
    "hidden_size = 256\n",
    "loss_da = nn.CrossEntropyLoss()\n",
    "\n",
    "aug_class(df, embed_size, hidden_size, loss_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 975,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, corpus_indices, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    data_iter = Data.DataLoader(corpus_indices, batch_size = 4, shuffle=True)\n",
    "        \n",
    "    for X, Y in data_iter:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if use_cuda:\n",
    "            X = Variable(torch.cuda.LongTensor(X))\n",
    "        else:\n",
    "            X = Variable(torch.LongTensor(X))\n",
    "        \n",
    "        y_pred = model(X.permute(1, 0)).squeeze(1)\n",
    "\n",
    "#         print('y_pred', y_pred)\n",
    "        loss = criterion(y_pred,Y.to(device).float())\n",
    "        acc = binary_accuracy(y_pred, Y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / len(data_iter), epoch_acc / len(data_iter)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 977,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    data_iter = Data.DataLoader(iterator, batch_size = 4, shuffle=True)\n",
    "    for X, Y in data_iter:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X.permute(1, 0)).squeeze(1)\n",
    "\n",
    "        loss = criterion(y_pred,Y.to(device).float())\n",
    "        acc = binary_accuracy(y_pred, Y)\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / len(data_iter), epoch_acc / len(data_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 978,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model_for_ori, model_for_aug, traing_data_ori, traing_data_aug, validation_data, optimizer, criterion, num_epochs):\n",
    "\n",
    "    N_EPOCHS = num_epochs\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss_ori, train_acc_ori = train(model_for_ori, traing_data_ori, optimizer, criterion)\n",
    "        valid_loss_ori, valid_acc_ori = evaluate(model_for_ori, validation_data, criterion)\n",
    "        train_loss_aug, train_acc_aug = train(model_for_aug, traing_data_aug, optimizer, criterion)\n",
    "        valid_loss_aug, valid_acc_aug = evaluate(model_for_aug, validation_data, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if valid_loss_aug < best_valid_loss:\n",
    "            best_valid_loss = valid_loss_aug\n",
    "            torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "            \n",
    "        if (epoch + 1) % 2 == 0:\n",
    "#             print('epoch %d, perplexity %f, time %.2f sec' % (\n",
    "#                 epoch + 1, perplexity, time.time() - start))\n",
    "\n",
    "            print(f'Epoch: {epoch+1:2} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "            print('Before data augmentation')\n",
    "            print(f'\\tTrain Loss: {train_loss_ori:.3f} | Train Acc: {train_acc_ori*100:.2f}%')\n",
    "            print(f'\\t Val. Loss: {valid_loss_ori:.3f} |  Val. Acc: {valid_acc_ori*100:.2f}%')\n",
    "            print('After data augmentation')\n",
    "            print(f'\\tTrain Loss: {train_loss_aug:.3f} | Train Acc: {train_acc_aug*100:.2f}%')\n",
    "            print(f'\\t Val. Loss: {valid_loss_aug:.3f} |  Val. Acc: {valid_acc_aug*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus_indices_ori' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-ec484164cfbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_for_ori\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_for_aug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m validation(model_for_ori, model_for_aug,corpus_indices_ori, corpus_indices_da, \n\u001b[0m\u001b[1;32m      4\u001b[0m            \u001b[0mcorpus_indices_classification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m            criterion , 6)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus_indices_ori' is not defined"
     ]
    }
   ],
   "source": [
    "model_for_ori = model\n",
    "model_for_aug = model\n",
    "validation(model_for_ori, model_for_aug,corpus_indices_ori, corpus_indices_da, \n",
    "           corpus_indices_classification, optimizer, \n",
    "           criterion , 6)\n",
    "# validation(model, corpus_indices_da, corpus_indices_classification, optimizer, criterion, 6, 'Data augmentation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-165-5b902677d7d6>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-165-5b902677d7d6>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    train(model, corpus_indices_da, optimizer, criterion)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# train(model, corpus_indices_da, optimizer, criterion)\n",
    "#     valid_loss, valid_acc = evaluate(model, corpus_indices_classification, criterion)\n",
    "    train(model, corpus_indices_da, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, corpus_indices_classification, criterion)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
