{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0 cpu\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import csv\n",
    "import torch.utils.data as Data\n",
    "import pandas as pd\n",
    "import math\n",
    "import torchtext.vocab as Vocab\n",
    "import collections\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext import data, datasets\n",
    "PAD, BOS, EOS = '<pad>', '<bos>', '<eos>'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# torch.set_printoptions(profile=8)\n",
    "print(torch.__version__, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将一个序列中所有的词记录在all_tokens中以便之后构造词典，然后在该序列后面添加PAD直到序列\n",
    "# 长度变为max_seq_len，然后将序列保存在all_seqs中\n",
    "def process_one_seq(seq_tokens, all_tokens, all_seqs, max_seq_len):\n",
    "    all_tokens.extend(seq_tokens) # add seq_tokens into one list, like extend the list, [..., seq_tokens].\n",
    "#     print('all_tokens', len(all_tokens))\n",
    "    seq_tokens += [EOS] + [PAD] * (max_seq_len - len(seq_tokens) - 1)\n",
    "#     print('after add pad seq_tokens: ', seq_tokens)\n",
    "    all_seqs.append(seq_tokens) # add seq_tokens become a list element, [...,[seq_tokens]].\n",
    "#     print('all_seqs', all_seqs)\n",
    "\n",
    "# 使用所有的词来构造词典。并将所有序列中的词变换为词索引后构造Tensor\n",
    "def build_data(all_tokens, all_seqs):\n",
    "\n",
    "#   collections.Counter(), A counter is a container that stores elements as dictionary keys, and their counts are stored as dictionary values.\n",
    "    tokens_dic = collections.Counter(all_tokens) \n",
    "#     print('tokens_dic', tokens_dic)\n",
    "    vocab = Vocab.Vocab(tokens_dic, specials = [PAD, BOS, EOS])\n",
    "#     print('vocab', vocab)\n",
    "    indices = [[vocab.stoi[w] for w in seq] for seq in all_seqs] # use its vocab_dic to represent the sentence. \n",
    "#     print('length of indices', len(indices))\n",
    "#     print('indices', torch.tensor(indices))\n",
    "    return vocab, torch.tensor(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeString(s):\n",
    "    s = s.str.lower()\n",
    "    s = s.str.replace(r\"<br />\",r\" \")\n",
    "    # s = re.sub(' +',' ',s)\n",
    "    s = s.str.replace(r'(\\W)(?=\\1)', '')\n",
    "    s = s.str.replace(r\"([.!?])\", r\" \\1\")\n",
    "    s = s.str.replace(r\"[^a-zA-Z.!?]+\", r\" \")\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(max_seq_len, data):\n",
    "    # in和out分别是input和output的缩写\n",
    "    in_tokens, in_seqs = [], []\n",
    "    df = data.copy()\n",
    "    df['comment'] = normalizeString(df['comment'])\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "#     print(df[:10])\n",
    "    k = 10\n",
    "    target = []\n",
    "    for line in range(k):\n",
    "#         print('line', line)\n",
    "        in_seq_tokens = df['comment'][line].split(' ')\n",
    "\n",
    "        if len(in_seq_tokens) > max_seq_len - 1:\n",
    "            continue  # 如果加上EOS后长于max_seq_len，则忽略掉此样本\n",
    "\n",
    "        process_one_seq(in_seq_tokens, in_tokens, in_seqs, max_seq_len)\n",
    "        target.append(df.label[line])\n",
    "        \n",
    "    in_vocab, in_data = build_data(in_tokens, in_seqs) # in_tokens is the list where contains every word, in_seqs is a list where its element are the sentence in French.\n",
    "    return in_vocab, in_data, torch.tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./IMDB Dataset.csv\", names=['comment', 'label'], header=0, encoding='utf-8')\n",
    "df['label'] = df['label'].apply(lambda x: 1 if x=='positive' else 0)\n",
    "df_train, df_test = train_test_split(df, test_size=0.3, random_state=41)\n",
    "vocab, corpus_indices, label = read_data(300, df_train)\n",
    "vocab_valid, corpus_indices_valid, label_valid = read_data(300, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN for Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 886,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_indices.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, drop_prob = 0):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size, dropout=drop_prob)\n",
    "        self.dense = nn.Linear(hidden_size, vocab_size)\n",
    "        self.state = None\n",
    "\n",
    "    def forward(self, inputs, state): # inputs: (seq_len, batch), \n",
    "        \n",
    "        embedding = self.embedding(inputs.to(torch.int64))\n",
    "        Y, state = self.rnn(embedding, self.state)\n",
    "        output = self.dense(Y[-1,:,:]) # so this is why we need Y[:, -1, :] 最外层的hidden state, because it already contains all the information of last all words.\n",
    "        return output, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 5\n",
    "hidden_size = 256\n",
    "model = RNNModel(vocab_size, embed_size, hidden_size).to(device)\n",
    "loss_da = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iter_consecutive(corpus_indices, batch_size, num_steps, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    corpus_indices = torch.tensor(corpus_indices, device=device)\n",
    "#     print('corpus_indices', corpus_indices)\n",
    "    data_len = len(corpus_indices)\n",
    "#     print('data_len', data_len)\n",
    "    batch_len = data_len // batch_size\n",
    "#     print('batch_len', batch_len)\n",
    "    indices = corpus_indices[0: batch_size*batch_len].view(batch_size, batch_len)\n",
    "#     print('indices', indices)\n",
    "#     print('indices', indices.shape)\n",
    "    epoch_size = (batch_len - 1) // num_steps\n",
    "#     print('epoch_size', epoch_size)\n",
    "    for i in range(epoch_size):\n",
    "#         i = i * num_steps\n",
    "        X = indices[:, i: i + num_steps]\n",
    "        Y = indices[:, i + num_steps + 1] # 错1位取data\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_rnn_pytorch(model, vocab_size, device,\n",
    "                                corpus_indices, char_to_idx,\n",
    "                                num_epochs, num_steps, lr, clipping_theta,\n",
    "                                batch_size):\n",
    "    print(corpus_indices.shape)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    state = None\n",
    "    for epoch in range(num_epochs):\n",
    "        l_sum, n, start = 0.0, 0, time.time()\n",
    "        data_iter = data_iter_consecutive(corpus_indices, batch_size, num_steps, device) # 相邻采样\n",
    "#         print('data_iter_consecutive is no errors')\n",
    "        for X, Y in data_iter:\n",
    "            if state is not None:\n",
    "                # 使用detach函数从计算图分离隐藏状态, 这是为了\n",
    "                # 使模型参数的梯度计算只依赖一次迭代读取的小批量序列(防止梯度计算开销太大)\n",
    "                if isinstance (state, tuple): # LSTM, state:(h, c)  \n",
    "                    state = (state[0].detach(), state[1].detach())\n",
    "                else:   \n",
    "                    state = state.detach()\n",
    "            (output, state) = model(X.permute(1,0), state) # change X shape to (seq, batch_size) instead of (batch_size, seq)\n",
    "            # output 的形状是(batch_size, vocab) Y(batch_size)\n",
    "            l = loss_da(output, Y.long())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            l_sum += l.item() * Y.shape[0]\n",
    "            n += Y.shape[0]\n",
    "            \n",
    "    corpus = predict_rnn_pytorch(corpus_indices, pred_len, model, vocab_size, device)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rnn_pytorch(corpus, num_chars, model, vocab_size, device):\n",
    "\n",
    "    output = []\n",
    "    for i in range(3, corpus.shape[0], 4):\n",
    "        \n",
    "        X = corpus[i-3:i]\n",
    "\n",
    "        state = None\n",
    "        if state is not None:\n",
    "            if isinstance(state, tuple): # LSTM, state:(h, c)  \n",
    "                state = (state[0].to(device), state[1].to(device))\n",
    "            else:   \n",
    "                state = state.to(device)\n",
    "        X = torch.tensor(torch.unsqueeze(X,0), device=device).view(3, 1) # X is a list need to become a two demission array then could be change to(3,1 )(seq, batch_size)\n",
    "        (Y, state) = model(X, state)  # Y (batch_size, vocab)\n",
    "        corpus[i] = int(Y.argmax(dim=1).item())\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300])\n",
      "torch.Size([300])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gaojie/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/gaojie/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300])\n",
      "torch.Size([300])\n",
      "torch.Size([300])\n",
      "torch.Size([300])\n",
      "torch.Size([300])\n",
      "torch.Size([300])\n",
      "torch.Size([300])\n",
      "torch.Size([300])\n",
      "i 5, time 5.06 sec\n",
      "torch.Size([300])\n",
      "torch.Size([300])\n",
      "torch.Size([300])\n",
      "torch.Size([300])\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "corpus = corpus_indices\n",
    "\n",
    "for i in range(corpus.shape[0]):\n",
    "\n",
    "    corpus_indice, tokens_dic, vocab_size = corpus[i], vocab, len(vocab)\n",
    "    print(corpus_indice.shape)\n",
    "    num_epochs, batch_size, lr, clipping_theta, num_steps= 20, 8, 1e-3, 1e-2, 4 # 注意这里的学习率设置\n",
    "\n",
    "#     print('before corpus', corpus[i])\n",
    "    corpus[i] = train_and_predict_rnn_pytorch(model, vocab_size, device,\n",
    "                            corpus_indice, tokens_dic, num_epochs, num_steps, \n",
    "                            lr, clipping_theta, batch_size)\n",
    "    if (i+1) % 5 == 0:\n",
    "            print('i %d, time %.2f sec' % (\n",
    "                i+1, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_indices_ori = Data.TensorDataset(corpus_indices, label)\n",
    "corpus_indices_da = Data.TensorDataset(corpus, label)\n",
    "corpus_indices_classification = Data.TensorDataset(corpus_indices_valid, label_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    use_cuda = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    use_cuda = False\n",
    "vocabLimit = vocab_length\n",
    "input_dim = 300\n",
    "# max_sequence_len = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n"
     ]
    }
   ],
   "source": [
    "class Model(torch.nn.Module) :\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim) :\n",
    "        super(Model,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocabLimit+1, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim,hidden_dim)\n",
    "        self.linearOut = nn.Linear(hidden_dim,1)\n",
    "    def forward(self,inputs,hidden):\n",
    "        print('inputs is', inputs.shape[0])\n",
    "        assert 'this is '\n",
    "        x = self.embeddings(inputs) # embeding 太小会报错\n",
    "        print('after embedding')\n",
    "        lstm_out,lstm_h = self.lstm(x)\n",
    "        x = lstm_out[-1]\n",
    "        x = self.linearOut(x)\n",
    "# \t\tx = F.log_softmax(x)\n",
    "        return x,lstm_h\n",
    "    def init_hidden(self) :\n",
    "        if use_cuda:\n",
    "            return (Variable(torch.zeros(1, 1, self.hidden_dim)).cuda(),Variable(torch.zeros(1, 300, self.hidden_dim)).cuda())\n",
    "        else:\n",
    "            return (Variable(torch.zeros(1, 1, self.hidden_dim)),Variable(torch.zeros(1, 300, self.hidden_dim)))\n",
    "\n",
    "if use_cuda:\n",
    "\tmodel = Model(input_dim, 50, 100).cuda()\n",
    "else:\n",
    "\tmodel = Model(input_dim, 500, 100)\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 4\n",
    "\n",
    "torch.save(model.state_dict(), 'model' + str(0)+'.pth')\n",
    "print('starting training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = len(vocab)\n",
    "vocab_length_valid = len(vocab_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 947,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "        \n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 948,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = vocab_length\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 949,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 950,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, corpus_indices, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    data_iter = Data.DataLoader(corpus_indices, batch_size = 4, shuffle=True)\n",
    "        \n",
    "    for X, Y in data_iter:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        if use_cuda:\n",
    "            X = Variable(torch.cuda.LongTensor(X))\n",
    "        else:\n",
    "            X = Variable(torch.LongTensor(X))\n",
    "        \n",
    "        y_pred = model(X.permute(1, 0)).squeeze(1)\n",
    "\n",
    "#         print('y_pred', y_pred)\n",
    "        loss = criterion(y_pred,Y.to(device).float())\n",
    "        acc = binary_accuracy(y_pred, Y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / len(data_iter), epoch_acc / len(data_iter)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    data_iter = Data.DataLoader(iterator, batch_size = 4, shuffle=True)\n",
    "    for X, Y in data_iter:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X.permute(1, 0)).squeeze(1)\n",
    "\n",
    "        loss = criterion(y_pred,Y.to(device).float())\n",
    "        acc = binary_accuracy(y_pred, Y)\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / len(data_iter), epoch_acc / len(data_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model_for_ori, model_for_aug, traing_data_ori, traing_data_aug, validation_data, optimizer, criterion, num_epochs):\n",
    "\n",
    "    N_EPOCHS = num_epochs\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss_ori, train_acc_ori = train(model_for_ori, traing_data_ori, optimizer, criterion)\n",
    "        valid_loss_ori, valid_acc_ori = evaluate(model_for_ori, validation_data, criterion)\n",
    "        train_loss_aug, train_acc_aug = train(model_for_aug, traing_data_aug, optimizer, criterion)\n",
    "        valid_loss_aug, valid_acc_aug = evaluate(model_for_aug, validation_data, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "            \n",
    "        if (epoch + 1) % 2 == 0:\n",
    "#             print('epoch %d, perplexity %f, time %.2f sec' % (\n",
    "#                 epoch + 1, perplexity, time.time() - start))\n",
    "\n",
    "            print(f'Epoch: {epoch+1:2} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "            print('Before data augmentation')\n",
    "            print(f'\\tTrain Loss: {train_loss_ori:.3f} | Train Acc: {train_acc_ori*100:.2f}%')\n",
    "            print(f'\\t Val. Loss: {valid_loss_ori:.3f} |  Val. Acc: {valid_acc_ori*100:.2f}%')\n",
    "            print('After data augmentation')\n",
    "            print(f'\\tTrain Loss: {train_loss_aug:.3f} | Train Acc: {train_acc_aug*100:.2f}%')\n",
    "            print(f'\\t Val. Loss: {valid_loss_aug:.3f} |  Val. Acc: {valid_acc_aug*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 958,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2 | Epoch Time: 0m 0s\n",
      "Before data augmentation\n",
      "\tTrain Loss: 0.700 | Train Acc: 45.83%\n",
      "\t Val. Loss: 0.704 |  Val. Acc: 37.50%\n",
      "After data augmentation\n",
      "\tTrain Loss: 0.701 | Train Acc: 41.67%\n",
      "\t Val. Loss: 0.701 |  Val. Acc: 37.50%\n",
      "Epoch:  4 | Epoch Time: 0m 0s\n",
      "Before data augmentation\n",
      "\tTrain Loss: 0.697 | Train Acc: 45.83%\n",
      "\t Val. Loss: 0.699 |  Val. Acc: 37.50%\n",
      "After data augmentation\n",
      "\tTrain Loss: 0.704 | Train Acc: 37.50%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 37.50%\n",
      "Epoch:  6 | Epoch Time: 0m 0s\n",
      "Before data augmentation\n",
      "\tTrain Loss: 0.693 | Train Acc: 58.33%\n",
      "\t Val. Loss: 0.691 |  Val. Acc: 62.50%\n",
      "After data augmentation\n",
      "\tTrain Loss: 0.692 | Train Acc: 58.33%\n",
      "\t Val. Loss: 0.689 |  Val. Acc: 62.50%\n"
     ]
    }
   ],
   "source": [
    "model_for_ori = model\n",
    "model_for_aug = model\n",
    "validation(model_for_ori, model_for_aug,corpus_indices_ori, corpus_indices_da, \n",
    "           corpus_indices_classification, optimizer, \n",
    "           criterion , 6)\n",
    "# validation(model, corpus_indices_da, corpus_indices_classification, optimizer, criterion, 6, 'Data augmentation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(model, corpus_indices_da, optimizer, criterion)\n",
    "#     valid_loss, valid_acc = evaluate(model, corpus_indices_classification, criterion)\n",
    "    train(model, corpus_indices_da, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, corpus_indices_classification, criterion)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
