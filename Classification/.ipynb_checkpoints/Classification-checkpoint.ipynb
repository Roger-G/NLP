{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import csv\n",
    "import torch.utils.data as Data\n",
    "import pandas as pd\n",
    "import torchtext.vocab as Vocab\n",
    "import collections\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext import data, datasets\n",
    "PAD, BOS, EOS = '<pad>', '<bos>', '<eos>'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将一个序列中所有的词记录在all_tokens中以便之后构造词典，然后在该序列后面添加PAD直到序列\n",
    "# 长度变为max_seq_len，然后将序列保存在all_seqs中\n",
    "def process_one_seq(seq_tokens, all_tokens, all_seqs, max_seq_len):\n",
    "#     print('seq_tokens', seq_tokens)\n",
    "    all_tokens.extend(seq_tokens) # add seq_tokens into one list, like extend the list, [..., seq_tokens].\n",
    "#     print('all_tokens', len(all_tokens))\n",
    "    seq_tokens += [EOS] + [PAD] * (max_seq_len - len(seq_tokens) - 1)\n",
    "#     print('after add pad seq_tokens: ', seq_tokens)\n",
    "    all_seqs.append(seq_tokens) # add seq_tokens become a list element, [...,[seq_tokens]].\n",
    "#     print('all_seqs', all_seqs)\n",
    "\n",
    "# 使用所有的词来构造词典。并将所有序列中的词变换为词索引后构造Tensor\n",
    "def build_data(all_tokens, all_seqs):\n",
    "#     print('all_tokens', all_tokens)\n",
    "    \n",
    "#   collections.Counter(), A counter is a container that stores elements as dictionary keys, and their counts are stored as dictionary values.\n",
    "    tokens_dic = collections.Counter(all_tokens) \n",
    "#     print('tokens_dic', tokens_dic)\n",
    "    vocab = Vocab.Vocab(tokens_dic, specials = [PAD, BOS, EOS])\n",
    "#     print('vocab', vocab)\n",
    "    indices = [[vocab.stoi[w] for w in seq] for seq in all_seqs] # use its vocab_dic to represent the sentence. \n",
    "#     print('length of indices', len(indices))\n",
    "#     print('indices', torch.tensor(indices))\n",
    "    return vocab, torch.tensor(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeString(s):\n",
    "\ts = s.str.lower()\n",
    "\ts = s.str.replace(r\"<br />\",r\" \")\n",
    "\t# s = re.sub(' +',' ',s)\n",
    "\ts = s.str.replace(r'(\\W)(?=\\1)', '')\n",
    "\ts = s.str.replace(r\"([.!?])\", r\" \\1\")\n",
    "\ts = s.str.replace(r\"[^a-zA-Z.!?]+\", r\" \")\n",
    "\t\n",
    "\treturn s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(max_seq_len, data):\n",
    "    # in和out分别是input和output的缩写\n",
    "    in_tokens, in_seqs = [], []\n",
    "    df = data.copy()\n",
    "#     df = pd.read_csv(\"./IMDB Dataset.csv\", names=['comment', 'label'], header=0, encoding='utf-8')\n",
    "    df['comment'] = normalizeString(df['comment'])\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    print(df[:10])\n",
    "    k = 1000\n",
    "    target = []\n",
    "    for line in range(k):\n",
    "#         print('line', line)\n",
    "        in_seq_tokens = df['comment'][line].split(' ')\n",
    "#         print('in_seq_tokens', in_seq_tokens)\n",
    "#         print('len(in_seq_tokens)', len(in_seq_tokens))\n",
    "#         print('max_seq_len - 1', max_seq_len - 1)\n",
    "        if len(in_seq_tokens) > max_seq_len - 1:\n",
    "            continue  # 如果加上EOS后长于max_seq_len，则忽略掉此样本\n",
    "#         print('here', line)\n",
    "#         print('df.label[k]', k, df.label[line])\n",
    "        process_one_seq(in_seq_tokens, in_tokens, in_seqs, max_seq_len)\n",
    "        target.append(df.label[line])\n",
    "#     print('target', len(target))\n",
    "    in_vocab, in_data = build_data(in_tokens, in_seqs) # in_tokens is the list where contains every word, in_seqs is a list where its element are the sentence in French.\n",
    "#     print('in_data', len(in_data))\n",
    "\n",
    "    return in_vocab, Data.TensorDataset(in_data, torch.tensor(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             comment  label\n",
      "0  i would have given this movie a but i laughed ...      0\n",
      "1  since was only a toddler when this show origin...      1\n",
      "2  the japanese have probably the most sadistic m...      1\n",
      "3  to fight against the death penalty is a just c...      1\n",
      "4  for my humanities quarter project for school i...      1\n",
      "5  and it s only january still i m sure of it ! b...      0\n",
      "6  it could have been a marvelous story based on ...      0\n",
      "7  well what can i say . what the f k ? there rea...      0\n",
      "8  riotously cheesy lunacy about lava spewing fro...      0\n",
      "9  woof ! pretty boring and they might as well ha...      0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./IMDB Dataset.csv\", names=['comment', 'label'], header=0, encoding='utf-8')\n",
    "df['label'] = df['label'].apply(lambda x: 1 if x=='positive' else 0)\n",
    "df_train, df_test = train_test_split(df, test_size=0.3, random_state=41)\n",
    "vocab, corpus_indices = read_data(300, df_train)\n",
    "# vocab_valid, corpus_indices_valid = read_data(300, df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = len(vocab)\n",
    "vocab_length_valid = len(vocab_valid.freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    use_cuda = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    use_cuda = False\n",
    "vocabLimit = vocab_length\n",
    "input_dim = 300\n",
    "# max_sequence_len = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n"
     ]
    }
   ],
   "source": [
    "class Model(torch.nn.Module) :\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim) :\n",
    "        super(Model,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocabLimit+1, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim,hidden_dim)\n",
    "        self.linearOut = nn.Linear(hidden_dim,1)\n",
    "    def forward(self,inputs,hidden):\n",
    "        print('inputs is', inputs.shape[0])\n",
    "        assert 'this is '\n",
    "        x = self.embeddings(inputs) # embeding 太小会报错\n",
    "        print('after embedding')\n",
    "        lstm_out,lstm_h = self.lstm(x)\n",
    "        x = lstm_out[-1]\n",
    "        x = self.linearOut(x)\n",
    "# \t\tx = F.log_softmax(x)\n",
    "        return x,lstm_h\n",
    "    def init_hidden(self) :\n",
    "        if use_cuda:\n",
    "            return (Variable(torch.zeros(1, 1, self.hidden_dim)).cuda(),Variable(torch.zeros(1, 300, self.hidden_dim)).cuda())\n",
    "        else:\n",
    "            return (Variable(torch.zeros(1, 1, self.hidden_dim)),Variable(torch.zeros(1, 300, self.hidden_dim)))\n",
    "\n",
    "if use_cuda:\n",
    "\tmodel = Model(input_dim, 50, 100).cuda()\n",
    "else:\n",
    "\tmodel = Model(input_dim, 500, 100)\n",
    "\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epochs = 4\n",
    "\n",
    "torch.save(model.state_dict(), 'model' + str(0)+'.pth')\n",
    "print('starting training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        \n",
    "        assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "        \n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = vocab_length\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, corpus_indices, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    data_iter = Data.DataLoader(corpus_indices, batch_size = 4, shuffle=True)\n",
    "        \n",
    "    for X, Y in data_iter:\n",
    "        optimizer.zero_grad()\n",
    "        if use_cuda:\n",
    "            X = Variable(torch.cuda.LongTensor(X))\n",
    "        else:\n",
    "            X = Variable(torch.LongTensor(X))\n",
    "        \n",
    "        y_pred = model(X.permute(1, 0)).squeeze(1)\n",
    "\n",
    "#         print('y_pred', y_pred)\n",
    "        loss = criterion(y_pred,Y.to(device).float())\n",
    "        acc = binary_accuracy(y_pred, Y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / len(data_iter), epoch_acc / len(data_iter)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    data_iter = Data.DataLoader(corpus_indices, batch_size = 4, shuffle=True)\n",
    "    for X, Y in data_iter:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X.permute(1, 0)).squeeze(1)\n",
    "\n",
    "        loss = criterion(y_pred,Y.to(device).float())\n",
    "        acc = binary_accuracy(y_pred, Y)\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / len(data_iter), epoch_acc / len(data_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 | Epoch Time: 0m 36s\n",
      "\tTrain Loss: 0.695 | Train Acc: 49.52%\n",
      "\t Val. Loss: 0.693 |  Val. Acc: 51.40%\n",
      "Epoch:  2 | Epoch Time: 0m 34s\n",
      "\tTrain Loss: 0.695 | Train Acc: 51.00%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 51.18%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 2\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, corpus_indices, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, corpus_indices_valid, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:2} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
