{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"Fine-tuning BertMasked Model with labeled dataset\"\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, TensorDataset, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import trange\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "PYTORCH_PRETRAINED_BERT_CACHE = Path(os.getenv('PYTORCH_PRETRAINED_BERT_CACHE',\n",
    "                                               Path.home() / '.pytorch_pretrained_bert'))\n",
    "# from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE\n",
    "from transformers import BertForMaskedLM\n",
    "from transformers import BertTokenizer\n",
    "logger = logging.getLogger(__name__)\n",
    "absFilePath = os.path.abspath('')\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class InputExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class AugProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class AugProcessor():\n",
    "    \"\"\"Processor for dataset to be augmented.\"\"\"\n",
    "        \n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_csv(os.path.join(data_dir, \"train.csv\")), \"train\") # get_train_examples\n",
    "\n",
    "    def get_dev_examples(self, data_dir): # get_dev_examples\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_csv(os.path.join(data_dir, \"dev.csv\")), \"dev\")\n",
    "\n",
    "    @ staticmethod\n",
    "    def get_labels(name): # get_labels\n",
    "        \"\"\"add your dataset here\"\"\"\n",
    "        if name in ['toxic']:\n",
    "            return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            guid =\"%s-%s\" % (set_type, i)\n",
    "#             print('guid', guid)\n",
    "            text_a = line[1][0]\n",
    "            text_b = None\n",
    "            label = line[1][-1]\n",
    "#             print('label', label)\n",
    "            examples.append(\n",
    "                InputExample(guid, text_a, text_b, label))\n",
    "        return examples\n",
    "    \n",
    "    @staticmethod\n",
    "    def _read_csv(input_file, quotechar='\"'):\n",
    "        \"\"\"Reads a comma separated value file.\"\"\"\n",
    "        with open(input_file,\"r\",encoding='UTF-8') as f:\n",
    "            reader = csv.reader(\n",
    "                f,\n",
    "                delimiter=\",\",\n",
    "                quotechar=quotechar,\n",
    "                doublequote=True,\n",
    "                skipinitialspace=False,\n",
    "                )\n",
    "\n",
    "            lines = []\n",
    "            for line in enumerate(reader):\n",
    "\n",
    "                if line[0] == 16:\n",
    "                    break\n",
    "                lines.append(line)\n",
    "                    \n",
    "            # delete label and sentence\n",
    "            del lines[0]\n",
    "#             print('line', lines)\n",
    "        return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert_examples_to_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    label_map = {label : i for i, label in enumerate(label_list)}\n",
    "#     print('label_map', label_map)\n",
    "    features = []\n",
    "    dupe_factor = 5\n",
    "    masked_lm_prob = 0.15\n",
    "    rng = random.Random(123)\n",
    "    max_predictions_per_seq = 20\n",
    "    a = examples\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "#         print('example', example)\n",
    "        tokens_a = tokenizer.tokenize(example.text_a) # (aaaaa, 1), aaaa is text_a\n",
    "        tokens_b = None\n",
    "        if len(tokens_a) > max_seq_length - 2:  # maxlength = [cls]+token_length + [sep]\n",
    "            tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "#         print('label', example.label, len(example.label))\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] # tokens_a is a token of sentence\n",
    "#         print('label_map[example.label]', label_map[example.label])\n",
    "        s = example.label\n",
    "    \n",
    "        try:\n",
    "            label_id = label_map[s]\n",
    "        except KeyError:\n",
    "\n",
    "            label_id = label_map[s.strip()]\n",
    "#         print('lael_id', label_id)\n",
    "        segment_ids = [label_id] * len(tokens) # use this label to make label_id and segment_ids\n",
    "        masked_lm_labels = [0]*max_seq_length\n",
    "\n",
    "        cand_indexes = []\n",
    "        for (i, token) in enumerate(tokens):\n",
    "            if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "#                 print('i', i, token)\n",
    "                continue\n",
    "#                 print('continue')\n",
    "            cand_indexes.append(i)\n",
    "        rng.shuffle(cand_indexes) # 随机替换词为mask，为下面做准备\n",
    "        print('cand_indexes', cand_indexes)\n",
    "        len_cand = len(cand_indexes)\n",
    "        output_tokens = list(tokens) ## contain '[CLS]' and '[SEP]'\n",
    "        num_to_predict = min(max_predictions_per_seq, ##  max_predictions_per_seq=20 最多预测20个\n",
    "                             max(1, int(round(len(tokens) * masked_lm_prob)))) # 最多替换tokens的50%\n",
    "\n",
    "        masked_lms_pos = []\n",
    "        covered_indexes = set()\n",
    "#         print('num_to_predict', num_to_predict)\n",
    "        for index in cand_indexes: # cand_indexes 是除 [CLS] 和 ['SEP'] 的token 的index集合, 但是已经被shuffle了\n",
    "            if len(masked_lms_pos) >= num_to_predict:\n",
    "                break\n",
    "            if index in covered_indexes:\n",
    "#                 print('stop too')\n",
    "                continue\n",
    "            covered_indexes.add(index)\n",
    "            \n",
    "#             mask_pos = []\n",
    "            masked_token = None\n",
    "            # 80% of the time, replace with [MASK]，这个句子的token的80%换成[MASK], 10% keep original, 10% random \n",
    "            if rng.random() < 0.8:\n",
    "                masked_token = \"[MASK]\"\n",
    "                output_tokens[index] = masked_token # mask 位置的token\n",
    "                masked_lms_pos.append(index)\n",
    "#                 print('masked_lms_pos', masked_lms_pos)\n",
    "                masked_lm_labels[index] = 1 #被mask的位置的原来的token的ids\n",
    "#         print('masked_lm_labels', masked_lm_labels)\n",
    "#             else:\n",
    "#                 # 10% of the time, keep original\n",
    "# #                 print('rng.random()', rng.random())\n",
    "#                 if rng.random() < 0.5:\n",
    "#                     masked_token = tokens[index]\n",
    "#                 # 10% of the time, replace with random word\n",
    "#                 else:\n",
    "#                     masked_token = tokens[cand_indexes[rng.randint(0, len_cand - 1)]]\n",
    "#             print('ex_index', ex_index)\n",
    "#             print('index', index)\n",
    "#             print('masked_lm_labels', masked_lm_labels)\n",
    "#                 masked_lm_labels[index] = tokenizer.convert_tokens_to_ids([tokens[index]])[0] #被mask的位置的原来的token的ids\n",
    "#             print('masked_lm_labels and ', masked_lm_labels)\n",
    "#             output_tokens[index] = masked_token # mask 位置的token\n",
    "#             masked_lms_pos.append(index) ## 被mask的token 在这句话的位置\n",
    "            \n",
    "        init_ids = tokenizer.convert_tokens_to_ids(tokens) # original tokens\n",
    "        token_idx = masked_lms_pos\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(output_tokens) # 80% of the time, replace with [MASK], token的80%probability 换成[MASK], 10% keep original, 10% random \n",
    "#         print('token idx', token_idx)\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        init_ids += padding\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(init_ids) == max_seq_length\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        if ex_index < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                [str(x) for x in tokens]))\n",
    "            logger.info(\"init_ids: %s\" % \" \".join([str(x) for x in init_ids]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"masked_lm_labels: %s\" % \" \".join([str(x) for x in masked_lm_labels]))\n",
    "#         print('input_mask', input_mask)\n",
    "        features.append(\n",
    "                InputFeatures(init_ids=init_ids, # original tokens ids\n",
    "                              input_ids=input_ids, # have mask in the sentences and to ids\n",
    "                              input_mask=input_mask, # padding 0 and no padding 1\n",
    "                              segment_ids=segment_ids, # if label=='1' then seg_ids = 1* [length of sentence ]\n",
    "                              masked_lm_labels=masked_lm_labels,\n",
    "                              token_idx=token_idx)) # 被mask的位置的原来的token的ids\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class InputFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, init_ids, input_ids, input_mask, segment_ids, masked_lm_labels, token_idx):\n",
    "        self.init_ids = init_ids\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.masked_lm_labels = masked_lm_labels\n",
    "        self.token_idx = token_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove_wordpiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def remove_wordpiece(str):\n",
    "    if len(str) > 1:\n",
    "        for i in range(len(str) - 1, 0, -1):\n",
    "            if str[i] == '[PAD]':\n",
    "                str.remove(str[i])\n",
    "            elif len(str[i]) > 1 and str[i][0] == '#' and str[i][1] == '#':\n",
    "                str[i - 1] += str[i][2:]\n",
    "                str.remove(str[i])\n",
    "    return \" \".join(str[1:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def run_aug(train_number, args, save_every_epoch=False):\n",
    "    # Augment the dataset with your own choice of Processer\n",
    "    processors = {\n",
    "        \"toxic\": AugProcessor\n",
    "    }\n",
    "\n",
    "    task_name = args.task_name\n",
    "    if task_name not in processors:\n",
    "        raise ValueError(\"Task not found: %s\" % (task_name))\n",
    "    args.data_dir = os.path.join(absFilePath, args.data_dir, task_name)\n",
    "    print('data_dir', args.data_dir)\n",
    "    args.output_dir = os.path.join(absFilePath, args.output_dir, task_name)\n",
    "    print('output_dir', args.output_dir)\n",
    "\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    processor = processors[task_name]()\n",
    "    label_list = processor.get_labels(task_name)\n",
    "    print('label_list', label_list)\n",
    "    ''' change to hugging face version''' \n",
    "    tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case) \n",
    "\n",
    "    train_examples = None\n",
    "    num_train_steps = None\n",
    "    train_examples = processor.get_train_examples(args.data_dir)\n",
    "    #dev_examples = processor.get_dev_examples(args.data_dir)\n",
    "    #train_examples.extend(dev_examples)\n",
    "    num_train_steps = int(len(train_examples) / args.train_batch_size * args.num_train_epochs) \n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "#     MODEL_name = \"{}/BertForMaskedLM_aug{}_epoch_3\".format(task_name.lower(), task_name.lower())\n",
    "    model = BertForMaskedLM.from_pretrained(args.bert_model)\n",
    "#     model = load_model(MODEL_name) \n",
    "#     ''' change to hugging face version''' \n",
    "    model.to(device)\n",
    "#     print('model', model)\n",
    "    # Prepare optimizer\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "\n",
    "\n",
    "    global_step = 0\n",
    "    train_features = convert_examples_to_features(\n",
    "        train_examples, label_list, args.max_seq_length, tokenizer)\n",
    "    clear_output()\n",
    "    \n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "    logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "    logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "\n",
    "    all_init_ids = torch.tensor([f.init_ids for f in train_features], dtype=torch.long)\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "\n",
    "    all_masked_lm_labels = torch.tensor([f.masked_lm_labels for f in train_features], dtype=torch.long)\n",
    "    train_data = TensorDataset(all_init_ids, all_input_ids, all_input_mask, all_segment_ids, all_masked_lm_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "\n",
    "    MASK_id = tokenizer.convert_tokens_to_ids(['[MASK]'])[0]\n",
    "    origin_train_path = os.path.join(args.output_dir, \"train_origin.csv\")\n",
    "    save_train_path = os.path.join(args.output_dir, \"augmentation_trainn_{}.csv\".format(train_number))\n",
    "#     shutil.copy(origin_train_path, save_train_path)\n",
    "\n",
    "    for e in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n",
    "        #torch.cuda.empty_cache()\n",
    "        count = 0\n",
    "#         shutil.copy(origin_train_path, save_train_path)\n",
    "        save_train_file = open(save_train_path, 'a', encoding='UTF-8')\n",
    "        csv_writer = csv.writer(save_train_file, delimiter=',')\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            model.eval()\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "    #         print('batch', batch)\n",
    "            init_ids, _, input_mask, segment_ids, all_masked_lm_labels = batch\n",
    "            masked_idx = []\n",
    "            for i in all_masked_lm_labels.numpy():\n",
    "                s = np.nonzero(i)\n",
    "                masked_idx.append(s[0].tolist())\n",
    "    #         print('masked_idx_jie',type(masked_idx_jie))\n",
    "    #         print('init_ids', init_ids)\n",
    "            for ids, idx in zip(init_ids, masked_idx):\n",
    "#                 print('hi')\n",
    "#                 print('original sentence:',tokenizer.convert_ids_to_tokens(ids.tolist()))\n",
    "#                 print('original token:',tokenizer.convert_ids_to_tokens(ids[idx].tolist()))\n",
    "                ids[idx] = MASK_id\n",
    "            predictions = model(init_ids, input_mask)\n",
    "#             print('predictions', predictions)\n",
    "            \n",
    "            for ids, idx, preds, seg in zip(init_ids, masked_idx, predictions[0], segment_ids):\n",
    "#                 print('hello')\n",
    "#                 idx = masked_idx[int(index)]\n",
    "                _, indice = torch.topk(torch.softmax(preds[idx], -1), 1)\n",
    "                ids[idx] = torch.squeeze(indice, 1)\n",
    "#                 print('replace tokens:',tokenizer.convert_ids_to_tokens(ids[idx].tolist()))\n",
    "#                 print('replace sentence:',tokenizer.convert_ids_to_tokens(ids.tolist()))    \n",
    "                pred_str = tokenizer.convert_ids_to_tokens(ids.cpu().numpy())\n",
    "                pred_str = remove_wordpiece(pred_str)\n",
    "                count +=1\n",
    "                csv_writer.writerow([pred_str, seg[0].item()])\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_number = 0 ## this is made for differiate different augmentation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def main(train_number):\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Required parameters\n",
    "    parser.add_argument(\"--data_dir\", default=\"datasets\", type=str,\n",
    "                        help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n",
    "    parser.add_argument(\"--output_dir\", default=\"aug_data\", type=str,\n",
    "                        help=\"The output dir for augmented dataset\")\n",
    "    parser.add_argument(\"--bert_model\", default=\"bert-base-uncased\", type=str,\n",
    "                        help=\"The path of pretrained bert model.\")\n",
    "    parser.add_argument(\"--task_name\",default=\"toxic\",type=str,\n",
    "                        help=\"The name of the task to train.\")\n",
    "    parser.add_argument(\"--max_seq_length\", default=30, type=int,\n",
    "                        help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
    "                             \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "                             \"than this will be padded.\")\n",
    "    parser.add_argument(\"--do_lower_case\", default=True, action='store_true',\n",
    "                        help=\"Set this flag if you are using an uncased model.\")\n",
    "    parser.add_argument(\"--train_batch_size\", default=4, type=int,\n",
    "                        help=\"Total batch size for training.\")\n",
    "    parser.add_argument(\"--learning_rate\", default=5e-5, type=float,\n",
    "                        help=\"The initial learning rate for Adam.\")\n",
    "    parser.add_argument(\"--num_train_epochs\", default=1, type=float,\n",
    "                        help=\"Total number of training epochs to perform.\")\n",
    "    parser.add_argument(\"--warmup_proportion\", default=0.1, type=float,\n",
    "                        help=\"Proportion of training to perform linear learning rate warmup for. \"\n",
    "                             \"E.g., 0.1 = 10%% of training.\")\n",
    "    parser.add_argument('--seed', type=int, default=42,\n",
    "                        help=\"random seed for initialization\")\n",
    "#     parser.add_argument('--train_number', type=int, default=0,\n",
    "#                         help='count the number of augmentation')\n",
    "\n",
    "    args, unknown = parser.parse_known_args()\n",
    "#     print(args.data_dir)\n",
    "    run_aug( train_number, args, save_every_epoch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'main' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-88c873843642>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_number\u001b[0m \u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'main' is not defined"
     ]
    }
   ],
   "source": [
    "main(train_number)\n",
    "\n",
    "train_number += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
